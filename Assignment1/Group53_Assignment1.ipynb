{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 53\n",
    "\n",
    "Names : SREE LAKSHMI HIRANMAYEE KADALI, \n",
    "        VAMSI SRI NAGA MANIKANTA MURUKONDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import cluster\n",
    "from warnings import simplefilter\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"spambase.data\",names=range(57),dtype=np.float64)\n",
    "output_data = pd.read_csv(\"spambase.data\",names=[57])\n",
    "\n",
    "train_data = train_data.values.astype(float)\n",
    "output_data = output_data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer = KBinsDiscretizer(n_bins = 10, encode = 'ordinal', strategy = 'quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis Space\n",
    "hyp = []\n",
    "for i in range(57):\n",
    "    hyp.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing hypothesis space from dataset to perform algorithm 4.1, and then performing algorithm 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGG_CONJ(hyp,train_data):\n",
    "    for i in range(len(hyp)):\n",
    "            if(train_data[i] not in hyp[i]):\n",
    "                hyp[i].append(train_data[i])\n",
    "    return(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGG(hyp,train_data):\n",
    "    for i in range(len(train_data[0])):\n",
    "        hyp[i].append(train_data[0][i])\n",
    "    while(i<len(train_data)):\n",
    "        hyp = LGG_CONJ(hyp,train_data[i])\n",
    "        i += 1\n",
    "    return(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []  #Initializing empty list to store predictions\n",
    "\n",
    "def testing(train_data):\n",
    "    \"\"\"Testing the data\"\"\"\n",
    "    for i in range(len(train_data)):\n",
    "        count = 0\n",
    "        for j in range(len(train_data[i])):\n",
    "            if(train_data[i][j] in hyp[j]):\n",
    "                count += 1\n",
    "        if(count == len(train_data[i])):\n",
    "            l.append(1)\n",
    "        else: l.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_data,output_data):\n",
    "    \"\"\"Training the dataset\"\"\"\n",
    "    l = []\n",
    "    for i in range(len(train_data)):\n",
    "            if(output_data[i] == 1):\n",
    "                l.append(train_data[i])\n",
    "    return(LGG(hyp,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_space():\n",
    "      \"\"\"This function calculates hypothesis space size\"\"\"\n",
    "      for i in range(len(hyp)):\n",
    "        j = sys.getsizeof([i])\n",
    "      print(\"Hypothesis space size : \",j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(train_data):\n",
    "    \"\"\"This function calculates accuracy\"\"\"\n",
    "    x = 0\n",
    "    total_acc = 0\n",
    "    for i in range(2):\n",
    "        x += train_data[i][i]\n",
    "        for j in range(2):\n",
    "          total_acc += train_data[i][j]\n",
    "    x = x/total_acc\n",
    "    print(\"Accuracy: \",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Dividing dataset into training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_train1, train_data_test1, output_data_train, output_data_test = train_test_split(train_data, output_data, test_size = 0.3 , random_state = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam emails in dataset : 69\n"
     ]
    }
   ],
   "source": [
    "#Spam mails in train data\n",
    "count = 0\n",
    "for i in range(len(output_data_train)):\n",
    "    if(output_data_train[i] == 1):\n",
    "        count += 1\n",
    "print(\"Spam emails in dataset :\",round((count/1813)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis space : [[0.0, 1.0], [1.0, 0.0, 4.0, 3.0, 2.0], [0.0], [0.0, 1.0, 2.0, 3.0], [0.0, 2.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 2.0, 1.0], [0.0, 1.0], [2.0, 0.0, 3.0, 1.0, 5.0, 4.0], [0.0, 1.0], [0.0], [0.0], [0.0, 1.0, 2.0], [0.0, 2.0, 1.0], [0.0, 2.0, 1.0], [6.0, 3.0, 0.0, 7.0, 2.0, 4.0, 5.0, 1.0], [0.0], [4.0, 3.0, 5.0, 0.0, 1.0, 2.0], [0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 2.0, 1.0], [0.0, 1.0], [0.0], [0.0], [0.0], [0.0, 1.0], [0.0], [0.0], [0.0], [0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0, 1.0, 2.0], [0.0, 1.0], [0.0], [0.0], [0.0, 1.0], [1.0, 0.0, 3.0, 2.0, 4.0, 5.0], [0.0, 1.0], [0.0, 1.0, 4.0, 3.0, 2.0], [1.0, 2.0, 0.0, 3.0], [0.0, 1.0], [6.0, 7.0, 9.0, 1.0, 8.0, 4.0, 3.0, 5.0, 2.0, 0.0], [8.0, 9.0, 3.0, 4.0, 6.0, 7.0, 5.0, 2.0, 1.0, 0.0], [7.0, 6.0, 4.0, 3.0, 5.0, 8.0, 9.0, 2.0, 1.0, 0.0], [0.0]]\n",
      "Hypothesis space size :  64\n",
      "Accuracy:  0.5257060101375814\n"
     ]
    }
   ],
   "source": [
    "#To get the clear view of output by ignoring warnings\n",
    "simplefilter(action = 'ignore', category = FutureWarning)\n",
    "simplefilter(action = 'ignore', category = UserWarning)\n",
    "\n",
    "#standardization and discretization for training data and test data\n",
    "train_data_train2 = preprocessing.scale(train_data_train1)\n",
    "discretizer.fit(train_data_train2)\n",
    "train_data_train = discretizer.transform(train_data_train2)\n",
    "train_data_test2 = preprocessing.scale(train_data_test1)\n",
    "discretizer.fit(train_data_test2)\n",
    "train_data_test = discretizer.transform(train_data_test2)\n",
    "hyp = training(train_data_train,output_data_train) #hypothesis space\n",
    "testing(train_data_test)\n",
    "print(\"Hypothesis space :\",hyp)\n",
    "calculate_space() #printing hypothesis space size\n",
    "z = cluster.contingency_matrix(output_data_test, l, eps = None, sparse = False)\n",
    "metric(z) #printing accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
